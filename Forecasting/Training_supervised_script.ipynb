{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a30d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Random Seed:  9432\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "#from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "\n",
    "from model_architecture import *\n",
    "from losses import *\n",
    "from Dataloader.Dataloader import *\n",
    "\n",
    "\n",
    "\n",
    "######################## configure device ###############\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# verify that gpu is recognized\n",
    "print(device)\n",
    "\n",
    "################## Set random seem for reproducibility ##########\n",
    "manualSeed = 9432\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "########## interactive mode for plots ###################\n",
    "plt.ion()   \n",
    "%matplotlib inline\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3165df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Data Loader #######################################\n",
    "# This code is adapted from the code used for supervised fine-tuning on scans from a different scanner. \n",
    "# In these experiments, the validation set was kept fixed across folds.\n",
    "# For initial training, the 5 folds had different validation set, randomly sampled from training set\n",
    "val_data=validation_dataset() \n",
    "val_loader=DataLoader(dataset=val_data, batch_size=16, shuffle=False, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "fld=5\n",
    "train_data=train_dataset(fold=fld, prcnt=25, discard_converted=False)\n",
    "train_loader=DataLoader(dataset=train_data, batch_size=1, shuffle=True, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "################## Prepare the model  ############################### \n",
    "encoder_model=Encoder_Network2D()\n",
    "classifier_model=Classification_Network()\n",
    "\n",
    "encoder_model.to(device)\n",
    "classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab813f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_inference(val_loader):\n",
    "    encoder_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    t_inp=torch.from_numpy(np.array([6.0, 12.0, 18.0, 24.0, 30.0, 36.0])).to(device)\n",
    "    t_inp=(t_inp/36.0)\n",
    "    \n",
    "    gt_lst=[] # 2D array:  sample, 6 time-points\n",
    "    pred_lst=[]\n",
    "    risk_scr_lst=[]\n",
    "    tcnv_lst=[]\n",
    "    indctr_lst=[]\n",
    "    nm_lst=[]\n",
    "    \n",
    "    for i, sample in enumerate(val_loader):\n",
    "        img=sample['img'].to(device)\n",
    "        gt=sample['gt'] # 6-D  conversion within 0/6/12/18/24/30/36 month time-points\n",
    "        tcnv=sample['tcnv']\n",
    "        indctr=sample['indctr']\n",
    "        nm=sample['nm']\n",
    "        \n",
    "        img=rearrange(img, 'b 1 c h w -> b c h w')\n",
    "        \n",
    "        ### Forward Pass\n",
    "        with torch.no_grad():\n",
    "            ftr=encoder_model(img)                    # 2B,768\n",
    "            \n",
    "            tmp=t_inp.unsqueeze(dim=0).repeat(img.shape[0],1).to(dtype=torch.float32)\n",
    "            rsk, pred_logits=classifier_model(ftr, tmp) # 3B, 1  and 3B,1\n",
    "            pred=F.sigmoid(pred_logits)\n",
    "            del tmp\n",
    "        \n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        ################################################\n",
    "        \n",
    "        pred_lst.append(pred)\n",
    "        risk_scr_lst.append(rsk.detach().cpu().numpy()) # risk predicted for the current input scan\n",
    "        nm_lst.append(nm)\n",
    "        gt_lst.append(gt)\n",
    "        tcnv_lst.append(tcnv)\n",
    "        indctr_lst.append(indctr)\n",
    "        \n",
    "        del img, gt, tcnv, indctr, nm,rsk, pred\n",
    "    \n",
    "    encoder_model.train()\n",
    "    classifier_model.train()\n",
    "    \n",
    "    \n",
    "    pred_lst=np.concatenate(pred_lst, axis=0) # or stack?  # B,6\n",
    "    risk_scr_lst=np.concatenate(risk_scr_lst, axis=0)      # B,1\n",
    "    nm_lst=np.concatenate(nm_lst, axis=0)                  # B,\n",
    "    gt_lst=np.concatenate(gt_lst, axis=0)                  # B,6\n",
    "    tcnv_lst=np.concatenate(tcnv_lst, axis=0)              # B,\n",
    "    indctr_lst=np.concatenate(indctr_lst, axis=0)          # B,\n",
    "    \n",
    "    ### Sort the list which is used to define the index for the samples of each bootstrap re-sampling.\n",
    "    idx=np.argsort(nm_lst, axis=0)\n",
    "    pred_lst=pred_lst[idx,:]           # B,6\n",
    "    risk_scr_lst=risk_scr_lst[idx,:]   # B,1\n",
    "    nm_lst=nm_lst[idx]                 # B,\n",
    "    gt_lst=gt_lst[idx,:]               # B,6\n",
    "    tcnv_lst=tcnv_lst[idx]             # B,\n",
    "    indctr_lst=indctr_lst[idx]         # B,\n",
    "    del idx\n",
    "    \n",
    "    ### Check that the sorted nm_lst is same as the validation dataloader\n",
    "    flag=np.array_equal(nm_lst, val_data.nm_lst) # this is the order used to define bootstrap samplings\n",
    "    if flag==False:\n",
    "        print('the nm_lst doesnot match !')\n",
    "    \n",
    "    \n",
    "    ###### Now everything is sorted by name. So now, we can use the pre-saved indices for each bootstrap ###\n",
    "    indices=val_data.sampling_index\n",
    "    c_lst=[]\n",
    "    auc_lst=[]\n",
    "    for k in range(0, len(indices)): # No. of bootstrap re-samplings\n",
    "        idx=indices[k]\n",
    "        tmp_rsk=np.squeeze(risk_scr_lst[idx,:], axis=1) # B,\n",
    "        tmp_tcnv=tcnv_lst[idx]\n",
    "        tmp_indctr=indctr_lst[idx]\n",
    "        c_index = concordance_index_censored(tmp_indctr.astype(bool), tmp_tcnv, tmp_rsk)\n",
    "        c_index = c_index[0]\n",
    "        c_lst.append(c_index)\n",
    "        del c_index, tmp_indctr, tmp_tcnv, tmp_rsk\n",
    "        \n",
    "        tmp_gt=gt_lst[idx,:]\n",
    "        tmp_pred=pred_lst[idx,:]\n",
    "        auc=[]\n",
    "        for cls in range(0,6):\n",
    "            gt=tmp_gt[:, cls]\n",
    "            pred=tmp_pred[:, cls]\n",
    "            idx2=np.where(gt !=-1) # -1 implies GT is unavailable (eg. time after censoring has occured)\n",
    "            gt=gt[idx2]\n",
    "            pred=pred[idx2]\n",
    "            # roc_auc_score(y_true, y_score\n",
    "            auc.append(roc_auc_score(gt, pred))\n",
    "            del gt, pred, idx2\n",
    "            # suppose within timepoint 36 months but image is censored at 24 months. then lbl is -1, needs to be avoided\n",
    "        \n",
    "        auc=np.expand_dims(np.array(auc), axis=0)\n",
    "        auc_lst.append(auc) # list of 6-dim arrays\n",
    "        del auc, idx\n",
    "        \n",
    "    \n",
    "    auc_lst=np.concatenate(auc_lst, axis=0) # B,6\n",
    "    c_lst=np.array(c_lst)\n",
    "    \n",
    "    mean_concordance=np.mean(c_lst)\n",
    "    avg_auc=np.mean(auc_lst, axis=1) # avg across 6 time-points\n",
    "    mn_avg_auc=np.mean(avg_auc, axis=0) # avg across each sampling.\n",
    "    # confidence intervals np.percentile(c_lst, 0.95)   and 0.05\n",
    "    \n",
    "    print('\\n CI: '+str(mean_concordance)+'  AUC: '+str(mn_avg_auc))\n",
    "    metric=mean_concordance+mn_avg_auc # this has to be maximized\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2654ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(sample, optimizer, scheduler):\n",
    "    \n",
    "    ### 2 intra-subject image-pairs img1, img2 \n",
    "    img1=sample['img1'].to(device)   # B,3,224,224\n",
    "    img2=sample['img2'].to(device)   # B,3,224,224\n",
    "    \n",
    "    img1=rearrange(img1, '1 b 1 c h w -> b c h w')\n",
    "    img2=rearrange(img2, '1 b 1 c h w -> b c h w')\n",
    "    \n",
    "    # gt1,gt2: the GT conversion labels (0/1) for conversion within 6/12/18/24 months etc\n",
    "    gt1=torch.unsqueeze(torch.squeeze(sample['gt1'], dim=0), dim=1).to(device)  \n",
    "    gt2=torch.unsqueeze(torch.squeeze(sample['gt2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    # tcnv1,tcnv2: time-to-conversion for the scans img1 and img2\n",
    "    tcnv1=torch.unsqueeze(torch.squeeze(sample['tcnv1'], dim=0), dim=1).to(device)  # B,1\n",
    "    tcnv2=torch.unsqueeze(torch.squeeze(sample['tcnv2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    # indctr1, indctr2: (0/1) 0=> censored, 1=> converts at some point. indctr1=indctr2 (since both come from same eye)\n",
    "    indctr1=torch.unsqueeze(torch.squeeze(sample['indctr1'], dim=0), dim=1).to(device)  # B,1\n",
    "    indctr2=torch.unsqueeze(torch.squeeze(sample['indctr2'], dim=0), dim=1).to(device) \n",
    "    \n",
    "    # time-interval between the visits in which img1,img2 were imaged.\n",
    "    tintrvl=torch.unsqueeze(torch.squeeze(sample['tintrvl'], dim=0), dim=1).to(device)  # B,1\n",
    "    \n",
    "    ########################  Forward Pass ########################\n",
    "    B=img1.shape[0]\n",
    "    img=torch.cat((img1, img2), dim=0)   # 2B,3,H,W\n",
    "    \n",
    "    #### prediction ####\n",
    "    ftr=encoder_model(img)               # 2B,768\n",
    "    \n",
    "    # for first 2B time, evaluate current risk and predictions (at t=0).\n",
    "    # for last B scans, predict risk corresponding to img2 directly from img1\n",
    "    t=torch.cat((torch.zeros((2*B, 1)).to(device), tintrvl), dim=0) # 3B,1\n",
    "    \n",
    "    ftr=torch.cat((ftr, ftr[0:B, :]), dim=0)  # 3B, 768\n",
    "    rsk, pred_logits=classifier_model(ftr, t) # 3B, 1  and 3B,1\n",
    "    \n",
    "    ####################################### COMPUTE LOSSES ########################################\n",
    "    # gt1(O:N, 1:0);  gt2(0:N/2, 1:N/2);   tot 0:1.5N, 1:.5N\n",
    "    # gt3(0:N/2, 1:N/2)   \n",
    "    cls_loss=bce_loss_logits(pred_logits[0:(2*B), 0:1], torch.cat((gt1, gt2), dim=0)) \n",
    "    cls_loss=cls_loss + F.binary_cross_entropy_with_logits(pred_logits[(2*B):, 0:1], gt2) # Future pred from current\n",
    "    \n",
    "    #### Consistency Loss ####\n",
    "    cnstncy_loss=F.binary_cross_entropy_with_logits(pred_logits[(2*B):, 0:1], F.sigmoid(pred_logits[B:(2*B), 0:1]))\n",
    "    \n",
    "    ###################### Risk score ranking Loss ###########\n",
    "    rank_loss=my_risk_concordance_loss(rsk[0:(2*B), 0:1],\n",
    "                                       torch.cat((indctr1, indctr2), dim=0),\n",
    "                                       torch.cat((tcnv1, tcnv2), dim=0))\n",
    "        \n",
    "    loss=cls_loss+cnstncy_loss+rank_loss\n",
    "    ################# Backpropagation ###########################\n",
    "    # remove previously stored gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Compute Gradients\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    #optimizer.step([cls_loss, cnstncy_loss, rank_loss], [1, 1, 1], None)\n",
    "    optimizer.step()\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    ############  Return Loss for Displaying #####\n",
    "    cls_loss=cls_loss.detach().cpu().numpy()\n",
    "    cnstncy_loss=cnstncy_loss.detach().cpu().numpy()\n",
    "    rank_loss=rank_loss.detach().cpu().numpy()\n",
    "    \n",
    "    loss=loss.detach().cpu().numpy()\n",
    "    \n",
    "    return loss, cls_loss, cnstncy_loss, rank_loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complete():\n",
    "    mn_lr=10**(-6.0)\n",
    "    mx_lr=10**(-4.0)\n",
    "    \n",
    "    nupdates=200#1000 # no of batch updates in each epoch\n",
    "    max_epochs=200\n",
    "    max_patience=100 # Early stopping if validation metric doesnot improve in this many consecutive epochs\n",
    "    \n",
    "    max_metric=complete_inference(val_loader)\n",
    "    \n",
    "    ############################################################################################################\n",
    "    optimizer = torch.optim.AdamW(list(encoder_model.parameters()) + list(classifier_model.parameters()), lr=mn_lr, amsgrad=True)\n",
    "    scheduler=torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=mn_lr, max_lr=mx_lr, cycle_momentum=False,\n",
    "                                            step_size_up=nupdates//2, step_size_down=None, mode='triangular')\n",
    "    ############################################################################################################\n",
    "    \n",
    "    patience=0\n",
    "    data_iter = iter(train_loader) \n",
    "    for epochs in range(0, max_epochs):\n",
    "        run_loss=0 # total loss  \n",
    "        run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_cnstncy_loss=0 # MSE loss of ODE feature pred\n",
    "        run_rnk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        tic=time.time()\n",
    "        for i in range(0, nupdates): # batch updates in each round of training (epoch) \n",
    "            try:\n",
    "                sample = next(data_iter) \n",
    "            except StopIteration:\n",
    "                # StopIteration is thrown if dataset ends\n",
    "                # reinitialize data loader \n",
    "                data_iter = iter(train_loader)\n",
    "                sample = next(data_iter)\n",
    "        \n",
    "            \n",
    "            loss, cls_loss, cnstncy_loss, rank_loss, optimizer, scheduler=train_one_batch(sample, optimizer, scheduler)\n",
    "            del sample\n",
    "            \n",
    "                \n",
    "            run_loss=run_loss+loss\n",
    "            run_cls_loss=run_cls_loss+cls_loss\n",
    "            run_cnstncy_loss=run_cnstncy_loss+cnstncy_loss\n",
    "            run_rnk_loss=run_rnk_loss+rank_loss\n",
    "            \n",
    "            del loss, cls_loss, cnstncy_loss, rank_loss\n",
    "                \n",
    "            if (i+1) % 10== 0: # displays after every 10 batch updates\n",
    "                print (\"Epoch [{}/{}], Batch [{}/{}], Train Loss: {:.4f}, Classification: {:.4f}, CONSISTENCY: {:.4f}, RANKING: {:.4f}\"\n",
    "                       .format(epochs+1, max_epochs, i+1, nupdates, (run_loss/i), (run_cls_loss/i), (run_cnstncy_loss/i), (run_rnk_loss/i)), end =\"\\r\")\n",
    "        \n",
    "            \n",
    "        ### End of an epoch. Check validation loss\n",
    "        metric=complete_inference(val_loader) \n",
    "        toc=time.time()\n",
    "        print('\\n Val Metric: '+str(metric)+'  Last Epoch took '+str(toc-tic)+' seconds')\n",
    "        \n",
    "        \n",
    "        run_loss=0 # total loss  \n",
    "        run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_cnstncy_loss=0  # BCE loss of consistency in pred for ODE\n",
    "        run_rnk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        #### Early stopping\n",
    "        if metric>max_metric:\n",
    "            max_metric=metric\n",
    "            patience=0\n",
    "            print('Validation metric improved !')\n",
    "            torch.save({\n",
    "                        'encoder_state_dict': encoder_model.state_dict(),\n",
    "                        'classifier_state_dict': classifier_model.state_dict()\n",
    "            },'best_weight_fld'+str(fld)+'_metric'+str(metric)+'.pt')\n",
    "        else:\n",
    "            patience=patience+1\n",
    "            print('\\n Validation metric has not improved in last '+str(patience)+' epochs')\n",
    "            if patience>max_patience:\n",
    "                print('Early Stopping !')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ded1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete()\n",
    "\n",
    "#weight of 3 for + class\n",
    "bce_loss_logits=nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([3.0]).to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
